{
    "dataset_revision": "a75a0eb",
    "mteb_version": "1.12.79",
    "scores": {
        "standard": [
            {
                "hf_subset": "earth_science",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.40661,
                "ndcg_at_1": 0.38793,
                "ndcg_at_5": 0.38782,
                "ndcg_at_10": 0.40661,
                "ndcg_at_25": 0.4409,
                "ndcg_at_50": 0.45836,
                "ndcg_at_100": 0.47961,
                "map_at_1": 0.15281,
                "map_at_5": 0.29214,
                "map_at_10": 0.32211,
                "map_at_25": 0.34119,
                "map_at_50": 0.34653,
                "map_at_100": 0.34988,
                "Recall_at_1": 0.15281,
                "Recall_at_5": 0.37096,
                "Recall_at_10": 0.44747,
                "Recall_at_25": 0.54783,
                "Recall_at_50": 0.6025,
                "Recall_at_100": 0.68067,
                "precision_at_1": 0.38793,
                "precision_at_5": 0.24483,
                "precision_at_10": 0.16638,
                "precision_at_25": 0.08966,
                "precision_at_50": 0.05138,
                "precision_at_100": 0.03026,
                "mrr": 0.49463
            },
            {
                "hf_subset": "sustainable_living",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.20821,
                "ndcg_at_1": 0.16667,
                "ndcg_at_5": 0.18579,
                "ndcg_at_10": 0.20821,
                "ndcg_at_25": 0.25644,
                "ndcg_at_50": 0.28381,
                "ndcg_at_100": 0.30855,
                "map_at_1": 0.07341,
                "map_at_5": 0.13178,
                "map_at_10": 0.14709,
                "map_at_25": 0.17044,
                "map_at_50": 0.17835,
                "map_at_100": 0.18318,
                "Recall_at_1": 0.07341,
                "Recall_at_5": 0.19334,
                "Recall_at_10": 0.25813,
                "Recall_at_25": 0.39933,
                "Recall_at_50": 0.48313,
                "Recall_at_100": 0.57565,
                "precision_at_1": 0.16667,
                "precision_at_5": 0.10741,
                "precision_at_10": 0.08611,
                "precision_at_25": 0.06333,
                "precision_at_50": 0.04333,
                "precision_at_100": 0.02778,
                "mrr": 0.25995
            },
            {
                "hf_subset": "theoremqa_theorems",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.28147,
                "ndcg_at_1": 0.24615,
                "ndcg_at_5": 0.2521,
                "ndcg_at_10": 0.28147,
                "ndcg_at_25": 0.33322,
                "ndcg_at_50": 0.34706,
                "ndcg_at_100": 0.37107,
                "map_at_1": 0.13513,
                "map_at_5": 0.20769,
                "map_at_10": 0.22236,
                "map_at_25": 0.23964,
                "map_at_50": 0.24218,
                "map_at_100": 0.24473,
                "Recall_at_1": 0.13513,
                "Recall_at_5": 0.28,
                "Recall_at_10": 0.3659,
                "Recall_at_25": 0.54205,
                "Recall_at_50": 0.60077,
                "Recall_at_100": 0.73308,
                "precision_at_1": 0.24615,
                "precision_at_5": 0.11385,
                "precision_at_10": 0.06923,
                "precision_at_25": 0.04123,
                "precision_at_50": 0.02308,
                "precision_at_100": 0.01369,
                "mrr": 0.3291
            },
            {
                "hf_subset": "aops",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.15095,
                "ndcg_at_1": 0.11712,
                "ndcg_at_5": 0.12372,
                "ndcg_at_10": 0.15095,
                "ndcg_at_25": 0.19321,
                "ndcg_at_50": 0.23325,
                "ndcg_at_100": 0.26727,
                "map_at_1": 0.03523,
                "map_at_5": 0.07675,
                "map_at_10": 0.09301,
                "map_at_25": 0.10628,
                "map_at_50": 0.11539,
                "map_at_100": 0.12056,
                "Recall_at_1": 0.03523,
                "Recall_at_5": 0.12205,
                "Recall_at_10": 0.19069,
                "Recall_at_25": 0.29976,
                "Recall_at_50": 0.41994,
                "Recall_at_100": 0.54376,
                "precision_at_1": 0.11712,
                "precision_at_5": 0.08829,
                "precision_at_10": 0.07297,
                "precision_at_25": 0.04973,
                "precision_at_50": 0.03802,
                "precision_at_100": 0.02568,
                "mrr": 0.22459
            },
            {
                "hf_subset": "economics",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.1618,
                "ndcg_at_1": 0.12621,
                "ndcg_at_5": 0.13718,
                "ndcg_at_10": 0.1618,
                "ndcg_at_25": 0.19828,
                "ndcg_at_50": 0.22564,
                "ndcg_at_100": 0.25671,
                "map_at_1": 0.04046,
                "map_at_5": 0.07552,
                "map_at_10": 0.09228,
                "map_at_25": 0.11026,
                "map_at_50": 0.12158,
                "map_at_100": 0.12971,
                "Recall_at_1": 0.04046,
                "Recall_at_5": 0.12064,
                "Recall_at_10": 0.20053,
                "Recall_at_25": 0.32588,
                "Recall_at_50": 0.42971,
                "Recall_at_100": 0.5568,
                "precision_at_1": 0.12621,
                "precision_at_5": 0.08932,
                "precision_at_10": 0.08252,
                "precision_at_25": 0.06718,
                "precision_at_50": 0.05243,
                "precision_at_100": 0.03709,
                "mrr": 0.20214
            },
            {
                "hf_subset": "pony",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.01251,
                "ndcg_at_1": 0.00893,
                "ndcg_at_5": 0.01063,
                "ndcg_at_10": 0.01251,
                "ndcg_at_25": 0.01748,
                "ndcg_at_50": 0.02536,
                "ndcg_at_100": 0.03931,
                "map_at_1": 0.00069,
                "map_at_5": 0.00182,
                "map_at_10": 0.00237,
                "map_at_25": 0.00343,
                "map_at_50": 0.00417,
                "map_at_100": 0.00518,
                "Recall_at_1": 0.00069,
                "Recall_at_5": 0.00365,
                "Recall_at_10": 0.00751,
                "Recall_at_25": 0.02006,
                "Recall_at_50": 0.03544,
                "Recall_at_100": 0.06495,
                "precision_at_1": 0.00893,
                "precision_at_5": 0.01071,
                "precision_at_10": 0.01339,
                "precision_at_25": 0.015,
                "precision_at_50": 0.01357,
                "precision_at_100": 0.01339,
                "mrr": 0.05226
            },
            {
                "hf_subset": "stackoverflow",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.13949,
                "ndcg_at_1": 0.09402,
                "ndcg_at_5": 0.12582,
                "ndcg_at_10": 0.13949,
                "ndcg_at_25": 0.18171,
                "ndcg_at_50": 0.21332,
                "ndcg_at_100": 0.24804,
                "map_at_1": 0.02984,
                "map_at_5": 0.07094,
                "map_at_10": 0.08129,
                "map_at_25": 0.09789,
                "map_at_50": 0.10724,
                "map_at_100": 0.11415,
                "Recall_at_1": 0.02984,
                "Recall_at_5": 0.12004,
                "Recall_at_10": 0.16919,
                "Recall_at_25": 0.29831,
                "Recall_at_50": 0.40009,
                "Recall_at_100": 0.53566,
                "precision_at_1": 0.09402,
                "precision_at_5": 0.09744,
                "precision_at_10": 0.07521,
                "precision_at_25": 0.0547,
                "precision_at_50": 0.04068,
                "precision_at_100": 0.02915,
                "mrr": 0.20203
            },
            {
                "hf_subset": "leetcode",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.3107,
                "ndcg_at_1": 0.28169,
                "ndcg_at_5": 0.27756,
                "ndcg_at_10": 0.3107,
                "ndcg_at_25": 0.33837,
                "ndcg_at_50": 0.35812,
                "ndcg_at_100": 0.38201,
                "map_at_1": 0.15822,
                "map_at_5": 0.22503,
                "map_at_10": 0.24114,
                "map_at_25": 0.25005,
                "map_at_50": 0.25359,
                "map_at_100": 0.25625,
                "Recall_at_1": 0.15822,
                "Recall_at_5": 0.30376,
                "Recall_at_10": 0.39331,
                "Recall_at_25": 0.49073,
                "Recall_at_50": 0.58157,
                "Recall_at_100": 0.70974,
                "precision_at_1": 0.28169,
                "precision_at_5": 0.1169,
                "precision_at_10": 0.07394,
                "precision_at_25": 0.03746,
                "precision_at_50": 0.02183,
                "precision_at_100": 0.01317,
                "mrr": 0.38102
            },
            {
                "hf_subset": "biology",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.32092,
                "ndcg_at_1": 0.33981,
                "ndcg_at_5": 0.29487,
                "ndcg_at_10": 0.32092,
                "ndcg_at_25": 0.3739,
                "ndcg_at_50": 0.39723,
                "ndcg_at_100": 0.42518,
                "map_at_1": 0.10774,
                "map_at_5": 0.20261,
                "map_at_10": 0.23339,
                "map_at_25": 0.25882,
                "map_at_50": 0.26568,
                "map_at_100": 0.27017,
                "Recall_at_1": 0.10774,
                "Recall_at_5": 0.27118,
                "Recall_at_10": 0.35809,
                "Recall_at_25": 0.51404,
                "Recall_at_50": 0.5922,
                "Recall_at_100": 0.70969,
                "precision_at_1": 0.33981,
                "precision_at_5": 0.19612,
                "precision_at_10": 0.13592,
                "precision_at_25": 0.0765,
                "precision_at_50": 0.04466,
                "precision_at_100": 0.0266,
                "mrr": 0.43418
            },
            {
                "hf_subset": "theoremqa_questions",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.29899,
                "ndcg_at_1": 0.29756,
                "ndcg_at_5": 0.28468,
                "ndcg_at_10": 0.29899,
                "ndcg_at_25": 0.3204,
                "ndcg_at_50": 0.33334,
                "ndcg_at_100": 0.34124,
                "map_at_1": 0.16533,
                "map_at_5": 0.25298,
                "map_at_10": 0.26183,
                "map_at_25": 0.2692,
                "map_at_50": 0.27132,
                "map_at_100": 0.27226,
                "Recall_at_1": 0.16533,
                "Recall_at_5": 0.30354,
                "Recall_at_10": 0.33728,
                "Recall_at_25": 0.40842,
                "Recall_at_50": 0.45916,
                "Recall_at_100": 0.49783,
                "precision_at_1": 0.29756,
                "precision_at_5": 0.1239,
                "precision_at_10": 0.07073,
                "precision_at_25": 0.03434,
                "precision_at_50": 0.0198,
                "precision_at_100": 0.01078,
                "mrr": 0.34216
            },
            {
                "hf_subset": "robotics",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.12824,
                "ndcg_at_1": 0.13861,
                "ndcg_at_5": 0.12447,
                "ndcg_at_10": 0.12824,
                "ndcg_at_25": 0.13653,
                "ndcg_at_50": 0.1592,
                "ndcg_at_100": 0.18829,
                "map_at_1": 0.03644,
                "map_at_5": 0.07487,
                "map_at_10": 0.0832,
                "map_at_25": 0.08946,
                "map_at_50": 0.09404,
                "map_at_100": 0.09753,
                "Recall_at_1": 0.03644,
                "Recall_at_5": 0.1005,
                "Recall_at_10": 0.14134,
                "Recall_at_25": 0.18905,
                "Recall_at_50": 0.26827,
                "Recall_at_100": 0.39401,
                "precision_at_1": 0.13861,
                "precision_at_5": 0.07723,
                "precision_at_10": 0.05644,
                "precision_at_25": 0.03248,
                "precision_at_50": 0.02495,
                "precision_at_100": 0.01812,
                "mrr": 0.18847
            },
            {
                "hf_subset": "psychology",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.26585,
                "ndcg_at_1": 0.19802,
                "ndcg_at_5": 0.23465,
                "ndcg_at_10": 0.26585,
                "ndcg_at_25": 0.31089,
                "ndcg_at_50": 0.33154,
                "ndcg_at_100": 0.35679,
                "map_at_1": 0.09153,
                "map_at_5": 0.15645,
                "map_at_10": 0.18687,
                "map_at_25": 0.21277,
                "map_at_50": 0.22096,
                "map_at_100": 0.22533,
                "Recall_at_1": 0.09153,
                "Recall_at_5": 0.2313,
                "Recall_at_10": 0.33063,
                "Recall_at_25": 0.50925,
                "Recall_at_50": 0.57451,
                "Recall_at_100": 0.67515,
                "precision_at_1": 0.19802,
                "precision_at_5": 0.14455,
                "precision_at_10": 0.12277,
                "precision_at_25": 0.08119,
                "precision_at_50": 0.05386,
                "precision_at_100": 0.03297,
                "mrr": 0.29113
            }
        ]
    },
    "task_name": "BrightRetrieval"
}